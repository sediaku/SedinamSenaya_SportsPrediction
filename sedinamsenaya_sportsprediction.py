# -*- coding: utf-8 -*-
"""SedinamSenaya_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nfJx_uJpPII-qf3jLOHF3uU3nsnUb_FS
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

data = pd.read_csv("/content/drive/My Drive/male_players (legacy).csv")

data.info()

data.head()

threshold = 0.5
threshold_count = int(threshold * len(data))
print(threshold_count)
data_cleaned = data.dropna(axis = 1, thresh = len(data)-threshold_count)
data_cleaned.info()
data_cleaned.head()

data = data_cleaned

data.drop(columns = ['player_id', 'player_url', 'fifa_update_date','player_face_url'], inplace = True)
data.head()

obj_data = data.select_dtypes(include=['object'])
obj_data.head()

data = data.drop(columns = obj_data.columns)
data.head()

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imp = IterativeImputer(max_iter = 10, random_state = 0)
numeric_data = pd.DataFrame(np.round(imp.fit_transform(data)), columns = data.copy().columns)

numeric_data.head()

data_corr = data.corr()
data_corr['overall'].sort_values(ascending = False)

high_correlation = data_corr[abs(data_corr['overall']) > 0.4]
high_correlation['overall'].sort_values(ascending = False)

subset = numeric_data[["overall","potential","value_eur", "wage_eur","release_clause_eur", "passing", "dribbling", "physic", "movement_reactions", "mentality_composure","attacking_short_passing", "mentality_vision", "international_reputation", "skill_long_passing", "shooting", "power_shot_power", "age", "skill_ball_control", "skill_curve", "power_long_shots"]]
subset.head()

y = subset['overall']
x = subset.drop('overall', axis=1)

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scale = StandardScaler()
scaled=scale.fit_transform(x)
subdata = pd.DataFrame(scaled, columns = x.columns)

Xtrain,Xtest,Ytrain,Ytest=train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error,mean_squared_log_error
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_squared_error,mean_squared_log_error
import pickle as pkl

parameters = {
"n_estimators":[100,500,1000],
"max_depth":[1,2,3,4],
}
rf=RandomForestRegressor()
rg = GridSearchCV(rf, parameters)
rg.fit(Xtrain,Ytrain)

pkl.dump(rg, open('/content/' + rg.__class__.__name__ + '.pkl', 'wb'))

y_pred = rg.predict(Xtest)
print(rg.__class__.__name__, mean_squared_error(y_pred,Ytest), np.sqrt(mean_squared_error(y_pred,Ytest)))

xgb = XGBRegressor()
parameters ={
"n_estimators":[100,500],
 "max_depth": [1,2,3,4],
"learning_rate":[0.3, 0.1, 0.03],
}

xgb_cv = GridSearchCV(xgb, parameters)
xgb_cv.fit(Xtrain,Ytrain)

pkl.dump(xgb_cv, open('/content/' + xgb_cv.__class__.__name__ + '.pkl', 'wb'))

y_pred = xgb_cv.predict(Xtest)
print(xgb_cv.__class__.__name__, mean_squared_error(y_pred,Ytest), np.sqrt(mean_squared_error(y_pred,Ytest)))

gb = GradientBoostingRegressor()
parameters ={
"n_estimators":[100,500],
 "max_depth": [1,2,3,4],
}

model_gb = GridSearchCV(gb, parameters)
model_gb.fit(Xtrain,Ytrain)

pkl.dump(model_gb, open('/content/' + model_gb.__class__.__name__ + '.pkl', 'wb'))

y_pred = model_gb.predict(Xtest)
print(model_gb.__class__.__name__, mean_squared_error(y_pred,Ytest), np.sqrt(mean_squared_error(y_pred,Ytest)))

def data_prep(file_name):
  data = pd.read_csv(file_name)

  threshold = 0.5
  threshold_count = int(threshold * len(data))
  print(threshold_count)
  data_cleaned = data.dropna(axis = 1, thresh = len(data)-threshold_count)
  data=data_cleaned
  data.drop(columns = ['sofifa_id', 'player_url','player_face_url','club_logo_url','club_flag_url','nation_flag_url'], inplace = True)
  obj_data = data.select_dtypes(include=['object'])
  data = data.drop(columns = obj_data.columns)
  imp = IterativeImputer(max_iter = 10, random_state = 0)
  numeric_data = pd.DataFrame(np.round(imp.fit_transform(data)), columns = data.copy().columns)

  subset = numeric_data[["potential","value_eur", "wage_eur","release_clause_eur", "passing", "dribbling", "physic", "movement_reactions", "mentality_composure","attacking_short_passing", "mentality_vision", "international_reputation", "skill_long_passing", "shooting", "power_shot_power", "age", "skill_ball_control", "skill_curve", "power_long_shots"]]
  scale = StandardScaler()
  scaled=scale.fit_transform(subset)
  subdata = pd.DataFrame(scaled, columns = subset.columns)
  return subdata

sub_data = data_prep("/content/drive/My Drive/players_22-1.csv")

print(sub_data)

sub_data.tail()

y_pred = xgb_cv.predict(sub_data.tail())

y_pred

